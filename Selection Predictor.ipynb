{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310ce4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from util.utils import load_data_set, gen_batch_sequence, run_lstm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from model.word_embedding import WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d535aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset\n",
      "Loaded 56355 queries and 18585 tables\n"
     ]
    }
   ],
   "source": [
    "train_query, train_table = load_data_set('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c1f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_query_id = [34,56,12,43] #random\n",
    "ret_tup = gen_batch_sequence(train_query,train_table,selected_query_id, 0, len(selected_query_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404ced24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['player'], ['no', '.'], ['nationality'], ['position'], ['years', 'in', 'toronto'], ['school/club', 'team']]\n"
     ]
    }
   ],
   "source": [
    "batch_query = ret_tup[0]\n",
    "batch_table = ret_tup[1]\n",
    "gt_quety = ret_tup[4]\n",
    "col_num = ret_tup[2]\n",
    "print(batch_table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af5935f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['player'], ['no', '.'], ['nationality'], ['position'], ['years', 'in', 'toronto'], ['school/club', 'team']], [['name'], ['canton'], ['height', '(', 'meters', ')'], ['crest', 'length', '(', 'meters', ')'], ['type'], ['year', 'of', 'construction'], ['name', 'of', 'the', 'lake']], [['player'], ['no', '.'], ['nationality'], ['position'], ['years', 'in', 'toronto'], ['school/club', 'team']], [['year'], ['tournaments', 'played'], ['cuts', 'made', '*'], ['wins'], ['2nd'], ['top', '10s'], ['best', 'finish'], ['earnings', '(', '$', ')'], ['money', 'list', 'rank'], ['scoring', 'average'], ['scoring', 'rank']]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342af766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SELECT', 'years', 'in', 'toronto', 'FROM', 'table_', 'WHERE', 'player', 'EQL', 'marcus', 'banks'], ['SELECT', 'canton', 'FROM', 'table_', 'WHERE', 'name', 'EQL', 'grande', 'dixence'], ['SELECT', 'school/club', 'team', 'FROM', 'table_', 'WHERE', 'no', '.', 'EQL', '6'], ['SELECT', 'max', '(', '2nd', ')', 'FROM', 'table_', 'WHERE']]\n"
     ]
    }
   ],
   "source": [
    "print(gt_quety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73519a39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "word_emb = WordEmbedding('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5521cd48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(word_emb.bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de487086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('During which years was Marcus Banks in Toronto?', ['Player', 'No.', 'Nationality', 'Position', 'Years in Toronto', 'School/Club Team'], 'SELECT years in toronto FROM table_ WHERE player EQL marcus banks'), ('What is the canton of grande dixence?', ['Name', 'Canton', 'Height (meters)', 'Crest length (meters)', 'Type', 'Year of construction', 'Name of the Lake'], 'SELECT canton FROM table_ WHERE name EQL grande dixence'), ('What school did player number 6 come from?', ['Player', 'No.', 'Nationality', 'Position', 'Years in Toronto', 'School/Club Team'], 'SELECT school/club team FROM table_ WHERE no . EQL 6'), ('What time was the highest for 2nd finishers?', ['Year', 'Tournaments played', 'Cuts made*', 'Wins', '2nd', 'Top 10s', 'Best finish', 'Earnings ($)', 'Money list rank', 'Scoring average', 'Scoring rank'], 'SELECT max ( 2nd ) FROM table_ WHERE')]\n"
     ]
    }
   ],
   "source": [
    "q_seq, col_seq, col_num, ans_seq, query_seq, ground_truth_cond_seq, raw_data = gen_batch_sequence(train_query, train_table,selected_query_id,0,len(selected_query_id))\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81790abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_lengths\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_lengths' is not defined"
     ]
    }
   ],
   "source": [
    "input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13097228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  6, 11], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fba1c1",
   "metadata": {},
   "source": [
    "## Function for running LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6909140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 47, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_op, input_lens = word_emb.gen_x_batch(batch_query, batch_table)\n",
    "x_emb = bert_op.last_hidden_state\n",
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2061717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_op.pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e417519",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d289c417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(768, 50, num_layers=4, bidirectional=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(768,hidden_size = 100//2, num_layers = 4,bidirectional=True)\n",
    "\n",
    "lstm.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475e790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_n, (c_fwd, c_rev) = lstm(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ccf8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_n: torch.Size([4, 47, 100])\n",
      "c_wd: torch.Size([8, 47, 50])\n",
      "c_rev: torch.Size([8, 47, 50])\n"
     ]
    }
   ],
   "source": [
    "print(f\"h_n: {h_n.shape}\\nc_wd: {c_fwd.shape}\\nc_rev: {c_rev.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6931dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  in the code for the reference paper they sorted the inputs (acc. to the size)\n",
    "# TODO: Clean this function: \n",
    "def run_lstm(lstm, inp, inp_length, prev_hidden=None):\n",
    "    '''\n",
    "    Input: This function takes in 3 arguments \n",
    "        lstm : the name of the lstm variable that needs to be run\n",
    "        inp  : the input in the for [Batch size , num_tok, last_layer]\n",
    "        inp_length: an array that contains the length of each element in the batch size = batch size\n",
    "        pre_hidden: hidden layer values of the previous lstm layer\n",
    "    \n",
    "    Ouptut: \n",
    "        Same as nn.LSTM\n",
    "    '''\n",
    "    ret_h, ret_c = lstm(inp,prev_hidden)\n",
    "    return ret_h, ret_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49713437",
   "metadata": {},
   "source": [
    "## Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52409987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4,5,6]])\n",
    "x = x.squeeze()\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "426da097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.tensor([[[1,2,3,4],\n",
    "                  [5,6,7,8]],\n",
    "                [[9,10,11,12],\n",
    "                 [13,14,15,16]],\n",
    "                [[9,10,11,12],\n",
    "                 [13,14,15,16]]\n",
    "                ]).to('cuda')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "363cf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237597f",
   "metadata": {},
   "source": [
    "### Scalar attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54216a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_attention = nn.Linear(num_hidden, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e1f9d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47])\n"
     ]
    }
   ],
   "source": [
    "att_val = scalar_attention(h_n)\n",
    "att_val = att_val.squeeze()\n",
    "print(att_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13f3b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x_len = max(input_lens)\n",
    "for idx, num in enumerate(input_lens): # reduce the importance of 0 values\n",
    "    if num < max_x_len:\n",
    "        att_val[idx,num:] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bab90e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1) # Probability distribution for the attention values\n",
    "att = softmax(att_val)\n",
    "print(att.shape)\n",
    "\n",
    "for x in att:\n",
    "    assert int(x.sum().ceil().tolist()) == 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bfd01bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 47, 100])\n",
      "torch.Size([4, 47, 100])\n",
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "print(h_n.shape)\n",
    "att_matrix = att.unsqueeze(2).expand_as(h_n)\n",
    "print(att_matrix.shape)\n",
    "K_agg = (h_n*att_matrix).sum(1)\n",
    "print(K_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7c7cd356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 100])\n"
     ]
    }
   ],
   "source": [
    "print(att_matrix[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd7cda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_out = nn.Sequential(\n",
    "            nn.Linear(num_hidden,num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, 6)\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "36b12a91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0695,  0.1001,  0.1088,  0.0280,  0.0561, -0.0524],\n",
      "        [-0.0667,  0.0973,  0.1071,  0.0263,  0.0583, -0.0472],\n",
      "        [-0.0653,  0.0935,  0.1049,  0.0239,  0.0583, -0.0464],\n",
      "        [-0.0636,  0.0896,  0.1036,  0.0177,  0.0548, -0.0475]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agg_score = agg_out(K_agg)\n",
    "print(agg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324b457",
   "metadata": {},
   "source": [
    "## Column Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab056e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['player'],\n",
       "  ['no', '.'],\n",
       "  ['nationality'],\n",
       "  ['position'],\n",
       "  ['years', 'in', 'toronto'],\n",
       "  ['school/club', 'team']],\n",
       " [['name'],\n",
       "  ['canton'],\n",
       "  ['height', '(', 'meters', ')'],\n",
       "  ['crest', 'length', '(', 'meters', ')'],\n",
       "  ['type'],\n",
       "  ['year', 'of', 'construction'],\n",
       "  ['name', 'of', 'the', 'lake']],\n",
       " [['player'],\n",
       "  ['no', '.'],\n",
       "  ['nationality'],\n",
       "  ['position'],\n",
       "  ['years', 'in', 'toronto'],\n",
       "  ['school/club', 'team']],\n",
       " [['year'],\n",
       "  ['tournaments', 'played'],\n",
       "  ['cuts', 'made', '*'],\n",
       "  ['wins'],\n",
       "  ['2nd'],\n",
       "  ['top', '10s'],\n",
       "  ['best', 'finish'],\n",
       "  ['earnings', '(', '$', ')'],\n",
       "  ['money', 'list', 'rank'],\n",
       "  ['scoring', 'average'],\n",
       "  ['scoring', 'rank']]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688cc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = col_seq\n",
    "ret = []\n",
    "col_len = np.zeros(len(cols), dtype=np.int64)\n",
    "names = []\n",
    "for b, one_cols in enumerate(cols):\n",
    "    names = names + one_cols\n",
    "    col_len[b] = len(one_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f1b34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  6, 11])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6421de00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b4fe18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_encoder = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_encoder)\n",
    "bert_model = BertModel.from_pretrained(bert_encoder)\n",
    "bert_args = {'padding': 'longest','return_tensors': 'pt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c22d95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.batch_encode_plus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_col_batch(cols):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5920521",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2094188904.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    inp_encode = bert_tokenizer.batch_encode_plus(batch_text_or_text_pairs=,padding='longest',return_tensors= 'pt')\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def gen_col_batch(cols):\n",
    "    bert_args = {'add_special_tokens': False,\n",
    "                  'return_token_type_ids': False,\n",
    "                  'padding': 'longest',\n",
    "                  'return_attention_mask': False,\n",
    "                  'return_tensors': 'pt'}\n",
    "    ret = []\n",
    "    col_len = np.zeros(len(cols), dtype=np.int64)\n",
    "    names = [z for x in cols for y in x for z in y]\n",
    "   \n",
    "    t = 0\n",
    "    print(len(cols))\n",
    "    for x in cols:\n",
    "        t+=len(x)\n",
    "    print(t)\n",
    "    \n",
    "    for b, one_cols in enumerate(cols):\n",
    "        #print(one_cols[0])\n",
    "        #names.append([y for x in one_cols for y in x])\n",
    "        col_len[b] = len(one_cols)\n",
    "    #print(len(names))  \n",
    "    for x in cols:\n",
    "        for y in x:\n",
    "            inp_encode = bert_tokenizer.batch_encode_plus(batch_text_or_text_pairs=,padding='longest',return_tensors= 'pt')\n",
    "    #print(inp_encode['input_ids'])\n",
    "    bert_output = bert_model(**inp_encode)\n",
    "    embs = bert_output.last_hidden_state\n",
    "    #print(names)\n",
    "    \n",
    "#     embs = []\n",
    "#     name_len = []\n",
    "#     tokenized_names = []\n",
    "#     tokenized_name_len = []\n",
    "#     tokenized_col_len = []\n",
    "    #for x in names:\n",
    "        \n",
    "    \n",
    "#     for x in names:\n",
    "#         inp_encode = bert_tokenizer.batch_encode_plus(batch_text_or_text_pairs=x,**bert_args)\n",
    "#         tokenized_names.append(inp_encode)\n",
    "#         tokenized_name_len.append(inp_encode['input_ids'].shape[1])\n",
    "#         tokenized_col_len.append(inp_encode['input_ids'].shape[0])\n",
    "#         #print(inp_encode['input_ids'])\n",
    "#         #print(tokenized_col_len)\n",
    "# #         for key in inp_encode.keys():\n",
    "# #             inp_encode[key] = inp_encode[key].to('cuda')\n",
    "#     max_len = max(tokenized_name_len)\n",
    "    \n",
    "    \n",
    "#     for i,x in enumerate(tokenized_names):\n",
    "#         #print([102] * (max_len - tokenized_name_len[i]))\n",
    "#         for j in tokenized_col_len:\n",
    "#             for k in range(j):\n",
    "#                 input_encode = x['input_ids'][k]\n",
    "#                 concat_list = [102] * (max_len - tokenized_name_len[i])\n",
    "#                 concat_tensor = torch.tensor(concat_list,dtype = torch.long)\n",
    "#                 #print(concat_tensor)\n",
    "#                 input_encode = torch.unsqueeze(torch.cat((input_encode,concat_tensor),dim=0),0)\n",
    "#                 #print(input_encode)\n",
    "#                 bert_op = bert_model(input_encode)\n",
    "#                 name_inp_var = bert_op.last_hidden_state\n",
    "#                 name_len.append(bert_op.last_hidden_state.shape[0])\n",
    "#                 embs.append(name_inp_var)\n",
    "#     max_len = max(name_len)\n",
    "    \n",
    "    #embs = torch.Tensor(embs).to('cuda') \n",
    "\n",
    "#     return embs, name_len, col_len\n",
    "    return embs,[0],col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc430cff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_col_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m col_inp_var, col_name_len, col_len \u001b[38;5;241m=\u001b[39m \u001b[43mgen_col_batch\u001b[49m(col_seq)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(col_inp_var[4].shape)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# for i in range(len(col_inp_var)):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     #if not col_inp_var[i].shape[1] == 3:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#         print(i,col_inp_var[i].shape)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(col_name_len)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(col_inp_var\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_col_batch' is not defined"
     ]
    }
   ],
   "source": [
    "col_inp_var, col_name_len, col_len = gen_col_batch(col_seq)\n",
    "# print(col_inp_var[4].shape)\n",
    "# for i in range(len(col_inp_var)):\n",
    "#     #if not col_inp_var[i].shape[1] == 3:\n",
    "#         print(i,col_inp_var[i].shape)\n",
    "# print(col_name_len)\n",
    "print(col_inp_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1161262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_inp_var[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0e343d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 25, 768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_inp_var[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "35c6a0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 18, 9, 22]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ba64aacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  6, 11], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3a9b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_lstm=nn.LSTM(input_size=768, hidden_size=100 // 2, num_layers=4, batch_first=True, \n",
    "                 dropout=0.3, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f52b6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(col_inp_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "54799b84",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m name_hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_inp_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_name_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Final Year Project\\Project\\Text2SQL-main\\util\\utils.py:70\u001b[0m, in \u001b[0;36mrun_lstm\u001b[1;34m(lstm, inp, inp_length, prev_hidden)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_lstm\u001b[39m(lstm, inp, inp_length, prev_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    This function takes in 3 arguments \u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    lstm : the name of the lstm variable that needs to be run\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    pre_hidden: hidden layer values of the previous lstm layer\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     ret_h, ret_c \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_h, ret_c\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\text2sql\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\text2sql\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:736\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     batch_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    737\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "name_hidden, _ = run_lstm(enc_lstm, col_inp_var, col_name_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f025c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "204820fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_col_batch(self, cols):\n",
    "        col_len = np.zeros(len(cols), dtype=np.int64)\n",
    "        names = []\n",
    "        for b, one_cols in enumerate(cols):\n",
    "            names = names + one_cols\n",
    "            col_len[b] = len(one_cols)\n",
    "            \n",
    "        name_inp_var = []\n",
    "        name_len = []\n",
    "        for x in names:\n",
    "            inp_encode = self.bert_tokenizer.batch_encode_plus(batch_text_or_text_pairs=x,**self.bert_args)\n",
    "            bert_op = self.bert_model(**inp_encode)\n",
    "            embs = bert_op.last_hidden_state\n",
    "            name_inp_var.append(embs)\n",
    "            name_len.append(bert_op.last_hidden_state.shape[0])\n",
    "        return name_inp_var, name_len, col_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c20c3",
   "metadata": {},
   "source": [
    "## Selection Prediction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name_encode(name_inp_var, col_len, enc_lstm,name_len=768):\n",
    "    # Encode the columns.\n",
    "    # The embedding of a column name is the last state of its LSTM output.\n",
    "    name, _ = run_lstm(enc_lstm, name_inp_var,name_len)\n",
    "    name_out = name_hidden[tuple(range(len(name_len))), name_len - 1]\n",
    "    ret = torch.FloatTensor(len(col_len), max(col_len), name_out.size()[1]).zero_()\n",
    "    if name_out.is_cuda:\n",
    "        ret = ret.cuda()\n",
    "\n",
    "    st = 0\n",
    "    for idx, cur_len in enumerate(col_len):\n",
    "        ret[idx, :cur_len] = name_out.data[st:st + cur_len]\n",
    "        st += cur_len\n",
    "    ret_var = Variable(ret)\n",
    "\n",
    "    return ret_var, col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e9336c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name_encode(name_inp_var, name_len, col_len, enc_lstm):\n",
    "    # Encode the columns.\n",
    "    # The embedding of a column name is the last state of its LSTM output.\n",
    "    ret_var, _ = run_lstm(enc_lstm, name_inp_var, name_len)\n",
    "    return ret_var, col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8eae0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectionClausePredictor(nn.Module):\n",
    "    def __init__(self,input_layer,hidden_size,num_layers,max_tok_num,gpu):\n",
    "        super(SelectionClausePredictor, self).__init__()\n",
    "        self.max_tok_num = max_tok_num\n",
    "        self.sel_lstm = nn.LSTM(input_size=input_layer, hidden_size=hidden_size // 2,\n",
    "                                num_layers=num_layers, batch_first=True,\n",
    "                                dropout=0.3, bidirectional=True)\n",
    "        \n",
    "        self.sel_att = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.sel_col_name_enc = nn.LSTM(input_size=input_layer, hidden_size=hidden_size // 2,\n",
    "                                num_layers=num_layers, batch_first=True,\n",
    "                                dropout=0.3, bidirectional=True)\n",
    "        self.sel_out_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.sel_out_col = nn.Linear(hidden_size, hidden_size)\n",
    "        self.sel_out = nn.Sequential(nn.Tanh(), nn.Linear(hidden_size, 1))\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        if(gpu):\n",
    "            self.sel_lstm = self.sel_lstm.to('cuda')\n",
    "            self.sel_col_name_enc = self.sel_col_name_enc.to('cuda')\n",
    "            self.sel_att  = self.sel_att.to('cuda')\n",
    "            self.softmax = self.softmax.to('cuda')\n",
    "            self.sel_out_K  = self.sel_out_K.to('cuda')\n",
    "            self.sel_out_col  = self.sel_out_col.to('cuda')\n",
    "            self.sel_out  = self.sel_out.to('cuda')\n",
    "    \n",
    "    def forward(self,x_input, x_len,col_inp_var, col_name_len, col_len,col_num):\n",
    "        \n",
    "        B = len(x_len)\n",
    "        max_x_len = max(x_len)\n",
    "        \n",
    "        e_col, _ = col_name_encode(col_inp_var, col_name_len, col_len, self.sel_col_name_enc)\n",
    "        h_enc, _ = run_lstm(self.sel_lstm, x_input, x_len) \n",
    "       \n",
    "        att_val = self.sel_att(h_n)  \n",
    "        att_val = att_val.squeeze()\n",
    "\n",
    "        for idx, num in enumerate(x_len):\n",
    "            if num < max_x_len:\n",
    "                att_val[idx, num:] = -100\n",
    "        \n",
    "        att = self.softmax(att_val)\n",
    "        K_sel = (h_enc * att.unsqueeze(2).expand_as(h_enc)).sum(1)\n",
    "        K_sel_expand = K_sel.unsqueeze(1)\n",
    "\n",
    "        sel_score = self.sel_out(self.sel_out_K(K_sel_expand) + self.sel_out_col(e_col)).squeeze()\n",
    "        max_col_num = max(col_num)\n",
    "        \n",
    "        for idx, num in enumerate(col_num):\n",
    "            if num < max_col_num:\n",
    "                sel_score[idx][num:] = -100\n",
    "\n",
    "        return sel_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b58424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb_var, x_len = word_emb.gen_x_batch(q_seq, col_seq)\n",
    "col_inp_var, col_name_len, col_len = word_emb.gen_col_batch(col_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2bd89b07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sel_pred \u001b[38;5;241m=\u001b[39m SelectionClausePredictor(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m200\u001b[39m,gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m----> 2\u001b[0m sel_score \u001b[38;5;241m=\u001b[39m \u001b[43msel_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_emb_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_inp_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_name_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_num\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\text2sql\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[128], line 32\u001b[0m, in \u001b[0;36mSelectionClausePredictor.forward\u001b[1;34m(self, x_input, x_len, col_inp_var, col_name_len, col_len, col_num)\u001b[0m\n\u001b[0;32m     29\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_len)\n\u001b[0;32m     30\u001b[0m max_x_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(x_len)\n\u001b[1;32m---> 32\u001b[0m e_col, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcol_name_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_inp_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_name_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel_col_name_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m h_enc, _ \u001b[38;5;241m=\u001b[39m run_lstm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msel_lstm, x_input, x_len) \n\u001b[0;32m     35\u001b[0m att_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msel_att(h_n)  \n",
      "Cell \u001b[1;32mIn[127], line 4\u001b[0m, in \u001b[0;36mcol_name_encode\u001b[1;34m(name_inp_var, name_len, col_len, enc_lstm)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcol_name_encode\u001b[39m(name_inp_var, name_len, col_len, enc_lstm):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Encode the columns.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# The embedding of a column name is the last state of its LSTM output.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     ret_var, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_inp_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_var, col_len\n",
      "File \u001b[1;32mD:\\Final Year Project\\Project\\Text2SQL-main\\util\\utils.py:70\u001b[0m, in \u001b[0;36mrun_lstm\u001b[1;34m(lstm, inp, inp_length, prev_hidden)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_lstm\u001b[39m(lstm, inp, inp_length, prev_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    This function takes in 3 arguments \u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    lstm : the name of the lstm variable that needs to be run\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    pre_hidden: hidden layer values of the previous lstm layer\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     ret_h, ret_c \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_h, ret_c\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\text2sql\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\text2sql\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:736\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     batch_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    737\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "sel_pred = SelectionClausePredictor(768, 100, 4,200,gpu=True) \n",
    "sel_score = sel_pred(x_emb_var, x_len, col_inp_var, col_name_len, col_len, col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b2e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
